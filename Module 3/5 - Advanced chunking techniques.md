## ğŸ¥ Video: Module 3 â€” Advanced Chunking Techniques  
**Instructor:** Zain Hassan  
**Video Duration:** ~5 minutes  
**Source:** [Coursera Video](https://www.coursera.org/learn/retrieval-augmented-generation-rag/lecture/pdpd9/advanced-chunking-techniques)

---

## ğŸ§  1. Motivation for Advanced Chunking

- Chunking improves retrieval, but **naive chunking risks losing context**.
- Example:  
  > â€œThat night she dreamed, as she did often, that she was finally an Olympic champion.â€  
  - A poorly placed split could misrepresent the meaning â€” making it seem like she **is** a champion, not dreaming of it.

- Fixed-size and recursive splitting **donâ€™t protect against semantic distortion**.

---

## ğŸ§  2. Semantic Chunking

### ğŸ” How It Works:
- Traverse the document **sentence by sentence**.
- For each sentence:
  - Compare its **vector** to the current chunkâ€™s vector.
  - If **distance < threshold**, add to current chunk.
  - If **distance â‰¥ threshold**, start a new chunk.

![alt text](<Ungraded Lab: Chunking/images/semantic chunking.png>)

### ğŸ“Š Visualization:
- A graph shows:
  - **Red line** = dissimilarity threshold
  - **Peak line** = semantic gap between current chunk and next sentence
  - When peak crosses red line â†’ new chunk begins

### âœ… Benefits:
- Chunks follow **train of thought**.
- Handles:
  - Conceptual tangents
  - Ideas spanning multiple paragraphs
- Smarter chunk boundaries
- Higher recall and presicion

### âš ï¸ Tradeoff:
- **Computationally expensive**:
  - Requires vectorizing every sentence and repeatedly calculating vectors for every sentence in your knowledge base.
  - Repeated comparisons


---

## ğŸ¤– 3. LLM-Based Chunking

### ğŸ§  How It Works:
- Pass document to a **language model** with instructions:
  - Group similar concepts
  - Split when topics change

### ğŸ§© Output:
- LLM generates chunks like it generates any other text.

### âœ… Benefits:
- High performance
- Flexible and adaptive

### âš ï¸ Tradeoff:
- **Black-box behavior**
- **Costly**, but becoming more viable as LLM prices drop

---

## ğŸ§  4. Context-Aware Chunking

### ğŸ§  How It Works:
- Ask LLM to:
  - Create chunks
  - Add **summary text** explaining each chunkâ€™s context

### ğŸ“Œ Example:
- A blog post ends with a list of names.
- LLM adds:  
  > â€œThis chunk contains acknowledgments from the author.â€

### âœ… Benefits:
- Improves:
  - **Vectorization** (semantic clarity)
  - **Retrieval relevance**
  - **LLM generation quality**

### âš ï¸ Tradeoff:
- Requires **LLM preprocessing** for every chunk
- **No impact on search speed**, but **high upfront cost**

---

## ğŸ§ª 5. When to Use What

| Technique               | Pros                                  | Cons                                  | Use Case                            |
|------------------------|----------------------------------------|---------------------------------------|-------------------------------------|
| Fixed-size / Recursive | Simple, fast, good for prototyping     | May split meaning                    | Default starting point              |
| Semantic Chunking      | Meaning-aware, improves retrieval      | Expensive, tuning required           | When precision matters              |
| LLM-Based Chunking     | Flexible, high performance             | Black-box, costly                    | When budget allows                  |
| Context-Aware Chunking | Enhances both retrieval & generation   | Preprocessing cost                   | First upgrade beyond fixed-size     |

---

## ğŸ¯ Final Takeaway

> â€œThe goal isnâ€™t to implement the most cutting-edge chunking technique. Itâ€™s to understand what options are available, how suitable they are to your data, and whether the costs and benefits make it worth implementing in your system.â€ â€” Zain Hassan