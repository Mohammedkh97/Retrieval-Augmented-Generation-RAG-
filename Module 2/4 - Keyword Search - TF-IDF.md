
## ğŸ¥ Video: Keyword Search â€“ TF-IDF  
**Instructor:** Zain Hassan  
**Duration:** ~7 minutes  
**Module:** 2 â€” Information Retrieval  
**Source:** [Coursera RAG Course](https://www.coursera.org/learn/retrieval-augmented-generation-rag/lecture/MYS0a/keyword-search-tf-idf)

---

## ğŸ§  What Is Keyword Search and Why Keyword Search Still Matters?

- Keyword search is a **foundational retrieval technique** used in databases and search engines for decades.
- It retrieves documents based on **shared words** between the prompt and the documents.
- The core assumption:  
  > â€œDocuments that contain more words from the prompt are more likely to be relevant.â€

---

## ğŸ§® Bag-of-Words Representation

- Both the prompt and each document are treated as a **bag of words**:
  - Word order is ignored.
  - Only word frequency matters.

### Example:
> â€œMaking pizza without a pizza ovenâ€  
- Word counts:  
  - `pizza`: 2  
  - `making`, `without`, `a`, `oven`: 1 each

- These counts are stored in a **sparse vector**:
  - Each position corresponds to a word in the vocabulary.
  - Most positions are zero â†’ hence â€œsparse.â€

---

## ğŸ—ƒï¸ Term-Document Matrix & Inverted Index

- A **sparse vector** is generated for each document.
- These vectors are arranged into a **term-document matrix**:
  - Rows = words  
  - Columns = documents

![alt text](<images/sparse vectors.png>)

- This matrix is also called an **inverted index**:
  - You start from a word and find all documents that contain it.
  - Opposite of traditional indexing (document â†’ words).

![alt text](<images/Inverted matrix.png>)

---

## âš™ï¸ Scoring Documents (Basic Keyword Match)

- When a prompt is submitted:
  - A sparse vector is generated for the prompt.
  - Each keyword is matched against the matrix.

### Scoring Logic:
- For each keyword:
  - Find its row in the matrix.
  - Award 1 point to every document that contains it.
- Total score = sum of keyword matches.
- Documents are ranked by score.

### Limitation:
- Doesnâ€™t account for **multiple occurrences** of a keyword.

---

## ğŸ” Frequency-Based Scoring

- To improve relevance:
  - Score is increased for **each occurrence** of a keyword.
  - Not just the first match.

### Problem:
- **Longer documents** naturally contain more keywords.
- This can **inflate scores unfairly**.

---

## ğŸ“ Normalization by Document Length

- To correct for document length bias:
  - Divide each documentâ€™s score by its total word count.
  - This produces a **normalized score**:
    - Rewards documents where keywords are a **larger share** of the text.
    - Penalizes long documents with diluted relevance.

---

## ğŸ“‰ Weighting by Rarity â€“ Inverse Document Frequency (IDF)

- Not all keywords are equally informative:
  - Common words (e.g., â€œtheâ€, â€œaâ€) appear in many documents.
  - Rare words (e.g., â€œpizzaâ€, â€œovenâ€) are more meaningful.

### IDF Calculation:
- For each word:
  - Count how many documents it appears in.
  - Divide by total number of documents â†’ Document Frequency (DF)
  - Invert the fraction â†’ Inverse Document Frequency (IDF)

### Example:
- `pizza` in 5 out of 100 docs â†’ DF = 0.05 â†’ IDF = 20  
- `the` in all 100 docs â†’ DF = 1 â†’ IDF = 1

> Rare words get higher IDF scores â†’ more weight in retrieval.

---

## ğŸ“‰ Log-Scaled IDF

- Raw IDF values can **overweight rare words**.
- To soften this:
  - Use **logarithmic scaling**:
    - Still favors rare words, but less aggressively.

---

## ğŸ§® TF-IDF Matrix Construction

- Multiply each wordâ€™s frequency in a document by its IDF score.
- This produces the **TF-IDF matrix**:
  - Rows = words  
  - Columns = documents  
  - Values = TF Ã— IDF

### Retrieval Logic:
- For each keyword in the prompt:
  - Traverse its row in the TF-IDF matrix.
  - Award each document the corresponding TF-IDF score.

---

## ğŸ§  TF-IDF Scoring Breakdown

The screen illustrates the **step-by-step transformation** from raw word counts to final TF-IDF scores using a term-document matrix. Here's how it unfolds:

---

### ğŸ”¹ Step 1: Build the Term-Document Matrix

Each document is represented as a **sparse vector** of word counts (term frequency). For example:

| Word     | Doc 1 | Doc 2 | Doc 3 |
|----------|-------|-------|-------|
| pizza    | 3     | 0     | 1     |
| oven     | 1     | 0     | 1     |
| the      | 5     | 4     | 6     |

This is the **raw term frequency (TF)** matrix.

---

### ğŸ”¹ Step 2: Compute Inverse Document Frequency (IDF)

For each word, calculate:

\[
\text{IDF}(w) = \log\left(\frac{N}{\text{DF}(w)}\right)
\]

Where:
- \( N \) = total number of documents
- \( \text{DF}(w) \) = number of documents containing word \( w \)

Example:
- `pizza` appears in 2 of 3 docs â†’ IDF = log(3/2) â‰ˆ 0.18
- `the` appears in all 3 docs â†’ IDF = log(3/3) = 0

---

### ğŸ”¹ Step 3: Multiply TF Ã— IDF

Each cell in the matrix is updated:

\[
\text{TF-IDF}_{i,j} = \text{TF}_{i,j} \times \text{IDF}_i
\]

Example for `pizza`:
- Doc 1: 3 Ã— 0.18 = 0.54  
- Doc 3: 1 Ã— 0.18 = 0.18

Now the matrix reflects **weighted importance** of each word in each document.

---

### ğŸ”¹ Step 4: Score Documents for a Prompt

Prompt:  
> â€œHow to make pizza in an ovenâ€

Extract keywords: `pizza`, `oven`, `make`

- For each keyword:
  - Look up its row in the TF-IDF matrix.
  - Sum the scores across documents.

Example scoring:
- Doc 1: pizza (0.54) + oven (0.2) + make (0.1) = **0.84**
- Doc 3: pizza (0.18) + oven (0.2) + make (0) = **0.38**
- Doc 2: all zeros = **0**

â†’ **Doc 1 is ranked highest**.

---

## âœ… Why This Matters in RAG

- TF-IDF helps **rank documents** by how well they match the prompt.
- It **boosts rare, meaningful words** and **downweights common ones**.
- Itâ€™s fast, interpretable, and a strong baseline for hybrid retrievers.

---

## ğŸ§ª Why TF-IDF Is a Strong Baseline

- TF-IDF balances:
  - **Term frequency** (how often a word appears)
  - **Inverse document frequency** (how rare the word is)

- Documents that:
  - Use keywords frequently  
  - Use **rare** keywords  
â†’ Score higher and are more likely to be relevant.

---

## ğŸ”„ Transition to BM25

- TF-IDF is foundational, but modern systems often use **BM25**:
  - A refined version that improves ranking behavior.
  - Handles term saturation and document length more robustly.

> â€œJoin me in the next video to learn how BM25 works and how keyword search fits into your RAG system.â€ â€” Zain Hassan

---

## ğŸ§­ Final Takeaway

TF-IDF is a **cornerstone of keyword-based retrieval**. Itâ€™s:
- Simple to implement
- Fast to compute
- Effective in many domains

But itâ€™s best used as part of a **hybrid retriever**, alongside semantic search and metadata filtering, to maximize relevance and robustness in RAG systems.
