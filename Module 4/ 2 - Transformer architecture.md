## ğŸ“˜ Notes: Module 4 â€” Transformer Architecture  
**Instructor:** Zain Hassan  
**Video Duration:** ~9 minutes  
**Source:** [Coursera Video](https://www.coursera.org/learn/retrieval-augmented-generation-rag/lecture/iY10m/transformer-architecture)

---

### ğŸ§  1. Why Transformers Matter in RAG

- In RAG, you send a prompt to an LLM and get a response grounded in retrieved info.
- But **why does this work**? How does the LLM understand the retrieved text?
- To answer that, we need to understand the **transformer architecture** that powers LLMs.

---

### ğŸ“œ 2. Origin of Transformers

- Introduced in the **2017 paper**: *Attention Is All You Need*.
- Originally designed for **machine translation**.
- Architecture includes:
  - **Encoder**: Understands source text (e.g., German).
  - **Decoder**: Generates target text (e.g., English).

> â€œMost LLMs only include the decoder component, since they focus on text generation.â€

---

### ğŸ§  3. Transformers in Embedding Models vs LLMs

- **Embedding models** use the encoder to create rich semantic representations.
- **LLMs** use the decoder to generate text based on input prompts.


---

### ğŸ§© 4. Prompt Journey Through a Transformer

#### ğŸ”¹ Tokenization
- Prompt is split into **tokens** (sub-word units).
- Each token gets:
  - A **dense vector** (first guess of meaning)
  - A **positional vector** (location in prompt)

> â€œThese guesses are static â€” the same token always gets the same initial vector.â€

![alt text](images/sentence.png)
![alt text](<images/sentence representation.png>)
---

#### ğŸ”¹ Attention Mechanism

- Each token looks at **every other token** in the prompt.
- It decides **which tokens to pay attention to**.
- Attention = which tokens most influence its meaning.

#### ğŸ§ª Example:
- Sentence: *â€œThe brown dog sat next to the red fox.â€*
- Token â€œdogâ€ might attend to:
  - â€œbrownâ€ (70%)
  - â€œsatâ€ (20%)
  - Others (10%)

> â€œAttention is a fancy way of saying which other tokens should have the biggest impact on my meaning.â€

---

#### ğŸ”¹ Attention Heads

- Each attention head specializes in a different type of relationship:
  - Descriptions (e.g., â€œfoxâ€ â†’ â€œbrownâ€)
  - Spatial relationships (e.g., â€œfoxâ€ â†’ â€œsatâ€, â€œnextâ€)

> â€œSmaller models use 8â€“16 heads; larger ones use 100+.â€

- Each token tracks relationships **multiple times**, each with a different focus.

---

#### ğŸ”¹ Feedforward Phase

  - Input Embeddings â†’ Initial token vectors
  - Attention â†’ Each token looks at others to decide relevance
  - Feed Forward â†’ Combines attention with original vector

- After attention, tokens enter a **feedforward network**.
- This updates each tokenâ€™s vector based on:
  - Original embedding
  - Position
  - Attention scores

![alt text](<images/feedforward base.png>)

> â€œThese are second guesses of each tokenâ€™s meaning â€” now informed by context.â€

##### Is Iterative Refinement:
In a transformer model, each token (like â€œdogâ€ or â€œsatâ€) starts with a first guess of its meaningâ€”based on its embedding and position. But that guess is not final. The model refines it through a series of layers, each consisting of:

- Attention: Determines which other tokens influence the current token.
- Feedforward network: Updates the tokenâ€™s vector based on attention scores and context.
- This process is repeated across 8 to 64 layers, depending on model size.

![alt text](<images/Iterative Refinement.png>)

---

### ğŸ§  5. Text Generation

- Once embeddings are refined:
  - Model predicts **next token** using a **probability distribution**.
  - Chooses one token based on probabilities.
  - Appends it to the prompt.
  - Repeats the entire process for each new token.

> â€œGenerating a single token takes a lot of processing.â€

---

### âš ï¸ 6. Implications for RAG

#### âœ… Why RAG Works
- LLMs deeply understand the **meaning and relevance** of retrieved info.
- Thanks to:
  - Attention mechanism
  - World knowledge in feedforward layers

#### âš ï¸ Randomness
- LLMs are **inherently random**.
- Even with good context, they may **ignore it** or generate unrelated text.

> â€œControlling this randomness is still necessary.â€

#### ğŸ’° Cost
- Transformers are **computationally expensive**.
- Cost grows with:
  - Prompt length
  - Completion length

> â€œMost costs in RAG come from running these powerful transformer models.â€

---

### ğŸ¯ Final Takeaway

- Transformers enable LLMs to:
  - Understand context
  - Track relationships
  - Generate coherent, grounded text
- This architecture is the **foundation** of why RAG systems work.

> â€œNow letâ€™s turn our attention â€” no pun intended â€” to how you can refine their behavior inside your RAG system.â€
